{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Download Reverse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sattwika/DownloaderScripts-Tools/blob/master/RecursiveGoIndexDownloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH7ZlsGKsQ0X",
        "colab_type": "text"
      },
      "source": [
        "## Recursive GoIndex Downloader\n",
        "\n",
        "**Features**\n",
        "*   Recursive crawler (**atlonxp**)\n",
        "*   Download all folders and files in a given url (**atlonxp**)\n",
        "*   Download all folders and files in in sub-folders (**atlonxp**)\n",
        "*   Adaptive delay in fetching url (**atlonxp**)\n",
        "*   Store folders/files directly to your Google Drive (**pankaj260**)\n",
        "*   Folders and files exclusion filters (**atlonxp**)\n",
        "*   Download queue supported (**atlonxp**)\n",
        "*   Auto-domain URL detection (**atlonxp**)\n",
        "*   API-based GoIndex crawler (**atlonxp**, **ifvv**)\n",
        "*   Parallel/Multiple files downloader (**atlonxp**)\n",
        "*   Auto-skip password-protected folders (**cxu-fork**)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHZKD2eIrSWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounting Google Drive, ignore this section if you don't want to\n",
        "# save on your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei98hPvrrlNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install dependencies\n",
        "!pip install requests tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki6wS0MCrnqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dependencies\n",
        "\n",
        "import json\n",
        "from json import JSONDecodeError\n",
        "\n",
        "import multiprocessing\n",
        "import os\n",
        "from pathlib import Path\n",
        "from random import randint\n",
        "from time import sleep\n",
        "from urllib import parse\n",
        "\n",
        "import requests\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ew87GUTHUu2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHOW_DOWNLOAD_PROGRESS = False\n",
        "OVERWRITE = False\n",
        "\n",
        "MIN_DELAY = 3\n",
        "MAX_DELAY = 5\n",
        "MAX_RETRY_CRAWLING = 5\n",
        "\n",
        "def check_exclusion(name, exclusions):\n",
        "    for exc in exclusions:\n",
        "        if exc in name:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def find(key, dictionary):\n",
        "    for k, v in dictionary.items():\n",
        "        if k == key:\n",
        "            yield v\n",
        "        elif isinstance(v, dict):\n",
        "            for result in find(key, v):\n",
        "                yield result\n",
        "        elif isinstance(v, list):\n",
        "            for d in v:\n",
        "                for result in find(key, d):\n",
        "                    yield result\n",
        "\n",
        "\n",
        "def crawler_v2(url, downloading_dict, path, level, exclusions, verbose=False):\n",
        "    # let slow down a bit\n",
        "    # sleep(randint(MIN_DELAY, MAX_DELAY))\n",
        "\n",
        "    url = parse.urlparse(url)\n",
        "    print(url.geturl())\n",
        "\n",
        "    try:\n",
        "        response_text = ''\n",
        "        retry = 0\n",
        "        while 'files' not in response_text:\n",
        "            retry += 1\n",
        "            if retry > MAX_RETRY_CRAWLING:\n",
        "                break\n",
        "            if retry > 1:\n",
        "                print('retry #{}'.format(retry), url.geturl())\n",
        "                sleep(randint(MIN_DELAY, MAX_DELAY))\n",
        "            data = {'password': task['password']} if 'password' in task else {}\n",
        "            response = requests.post(url.geturl(), data=data)\n",
        "            response_text = response.text\n",
        "        # print(response.text)\n",
        "        response_json = json.loads(response_text)\n",
        "    except JSONDecodeError:\n",
        "        sleep(randint(MIN_DELAY, MAX_DELAY))\n",
        "        print('- Data is missing! change a plan -')\n",
        "        print('- > use terminal CURL            -')\n",
        "        try:\n",
        "            response = os.popen(\"curl --globoff {} -d ''\".format(url.geturl())).read()\n",
        "            response_json = json.loads(response)\n",
        "        except Exception as e:\n",
        "            print('Nah, something went wrong!')\n",
        "            print(e.args())\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print('Nah, something went wrong!')\n",
        "        print(e.args())\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        files_dict = list(find('files', response_json))[0]\n",
        "    except Exception as e:\n",
        "        print('Cannot fine value for the key of \"files\", skip this link')\n",
        "        files_dict = {}\n",
        "\n",
        "    for file in files_dict:\n",
        "        name = file['name']\n",
        "\n",
        "        # if @name contains exclusion word, we ignore\n",
        "        if check_exclusion(name, exclusions):\n",
        "            continue\n",
        "\n",
        "        if 'folder' in file['mimeType']:\n",
        "            next_url = url.geturl() + parse.quote(name) + \"/\"\n",
        "            next_path = os.path.join(path, name)\n",
        "            downloading_dict = crawler_v2(next_url, downloading_dict, next_path, level + 1, exclusions, verbose)\n",
        "        else:\n",
        "            name = file['name']\n",
        "            domain_name = url.geturl()\n",
        "            if verbose:\n",
        "                print('  ' + name)\n",
        "            try:\n",
        "                downloading_dict.append({\n",
        "                    'folder': path,\n",
        "                    'filename': name,\n",
        "                    'filename_abs': os.path.join(path, name),\n",
        "                    'size': file['size'],\n",
        "                    'url': '{}{}{}'.format(domain_name, '/' if not domain_name.endswith('/') else '', parse.quote(name)),\n",
        "                })\n",
        "            except:\n",
        "                print('skipping', url.geturl() + parse.quote(name))\n",
        "                continue\n",
        "\n",
        "    # print(json.dumps(downloading_dict, indent=2), end='\\n\\n')\n",
        "    return downloading_dict\n",
        "\n",
        "\n",
        "def download_agent(task, OVERWRITE=OVERWRITE, METHOD='curl', SILENT=True):\n",
        "    folder = task['folder']\n",
        "    filename = task['filename']\n",
        "    filename_abs = task['filename_abs']\n",
        "    url = task['url']\n",
        "    size = int(task['size'])\n",
        "\n",
        "    result = {\n",
        "        'task': task\n",
        "    }\n",
        "\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        if not os.path.exists(filename_abs) or OVERWRITE:\n",
        "            pass\n",
        "        else:\n",
        "            # file exists, check file size\n",
        "            if os.path.getsize(filename_abs) >= size:\n",
        "                result.update({ 'status': -1 })\n",
        "                task = None\n",
        "            else:\n",
        "                pass\n",
        "        retry = False\n",
        "        if task:\n",
        "            sleep(randint(3, 10))\n",
        "            if METHOD is 'curl':\n",
        "                command = \"curl --globoff '{}' --output '{}' {}\".format(\n",
        "                    task['url'],\n",
        "                    task['filename_abs'],\n",
        "                    '-s' if SILENT else ''\n",
        "                    )\n",
        "                os.popen(command).read()\n",
        "                # check filesize again\n",
        "                if os.path.getsize(filename_abs) >= size:\n",
        "                    result.update({ 'status': 0 })\n",
        "                else:\n",
        "                    # error --> 404, user rate limit, etc. --> put in failures\n",
        "                    # list for re-downloading\n",
        "                    retry = True\n",
        "            else:\n",
        "                r = requests.get(url, stream=True)\n",
        "                if r.status_code is 200:\n",
        "                    with open(filename_abs, 'ab+') as f:\n",
        "                        f.write(r.content)\n",
        "                    # check filesize again\n",
        "                    if os.path.getsize(filename_abs) >= size:\n",
        "                        result.update({ 'status': 0 })\n",
        "                    else:\n",
        "                        # error --> user rate limit, etc. --> put in failures\n",
        "                        # list for re-downloading\n",
        "                        result.update({ 'status': 1 })\n",
        "                else:\n",
        "                    # error --> 404\n",
        "                    result.update({ 'status': 1 })\n",
        "\n",
        "        if retry:\n",
        "            r = requests.get(url, stream=True)\n",
        "            if r.status_code is 200:\n",
        "                with open(filename_abs, 'ab+') as f:\n",
        "                    f.write(r.content)\n",
        "                # check filesize again\n",
        "                if os.path.getsize(filename_abs) >= size:\n",
        "                    result.update({ 'status': 0 })\n",
        "                else:\n",
        "                    # error --> user rate limit, etc. --> put in failures\n",
        "                    # list for re-downloading\n",
        "                    result.update({ 'status': 1 })\n",
        "            else:\n",
        "                # error --> 404\n",
        "                result.update({ 'status': 1 })\n",
        "\n",
        "    except Exception as e:\n",
        "        print('[Exception]', e.args, task['url'])\n",
        "        result.update({ 'status': 1 })\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_filesize(size, power=3):\n",
        "    return size/pow(1024, power)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqwqZ8Qrr-6m",
        "colab_type": "code",
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "colab": {}
      },
      "source": [
        "exclusions = ['__MACOSX/']\n",
        "\n",
        "destination = \"/content/drive/My Drive/\"\n",
        "download_tasks = [\n",
        "    {\n",
        "        'folder': 'FrontEndMasters - Complete Intro to Containers',\n",
        "        'url': 'https://tutnetflix.mlwdl.workers.dev/FrontEndMasters%20-%20Complete%20Intro%20to%20Containers/'\n",
        "    },\n",
        "]\n",
        "\n",
        "print('##################################')\n",
        "print('# Crawling all downloadable urls #')\n",
        "print('##################################', end='\\n\\n')\n",
        "tasks = []\n",
        "for task in download_tasks:\n",
        "    tasks += crawler_v2(task['url'], [], os.path.join(destination, task['folder']), 0, exclusions, verbose=False)\n",
        "    # print(json.dumps(tasks, indent=2), end='\\n\\n')\n",
        "\n",
        "total_size = get_filesize(sum([int(task['size']) for task in tasks]))\n",
        "\n",
        "# print(json.dumps(tasks, indent=2))\n",
        "print('\\nTotal Task:', len(tasks))\n",
        "print('Total size: %.3fGB' % total_size, end='\\n\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjpVT5B_to7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOWNLOAD_TASKS = 32\n",
        "\n",
        "print('######################################')\n",
        "print('# Downloading {} files and folders  #'.format(len(tasks)))\n",
        "print('######################################', end='\\n\\n')\n",
        "pool = multiprocessing.Pool(processes=MAX_DOWNLOAD_TASKS)  # Num of CPUs\n",
        "\n",
        "downloads = []\n",
        "skips = []\n",
        "failures = []\n",
        "errors = []\n",
        "downloaded_size = 0\n",
        "with tqdm.tqdm(total=len(tasks)) as pbar:\n",
        "    for i, result in enumerate(pool.imap_unordered(download_agent, tasks)):\n",
        "        if result is not None:\n",
        "            status = result.get('status')\n",
        "            task = result.get('task')\n",
        "            downloaded = int(task['size'])\n",
        "            if status == 0:\n",
        "                downloaded_size += downloaded\n",
        "                pbar.set_description('[%.3f/%.3f GB] Downloading %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                downloads.append(task)\n",
        "            elif status == 1:\n",
        "                pbar.set_description('[%.3f/%.3f GB] Fail downloading %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                failures.append(task)\n",
        "            elif status == -1:\n",
        "                downloaded_size += downloaded\n",
        "                pbar.set_description('[%.3f/%.3f GB] Skipping %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                skips.append(task)\n",
        "            else:\n",
        "                pass\n",
        "        else:\n",
        "            pbar.set_description('[%.3f/%.3f GB] ERROR')\n",
        "        pbar.update()\n",
        "\n",
        "print('Waiting 1 minute')\n",
        "sleep(60)\n",
        "\n",
        "# print(json.dumps(failures, indent=2))\n",
        "if len(failures) > 0:\n",
        "    print('\\n\\n##################################')\n",
        "    print('# Retry all {} failures          #'.format(len(failures)))\n",
        "    print('##################################')\n",
        "    with tqdm.tqdm(total=len(failures)) as pbar:\n",
        "        for i, result in enumerate(pool.imap_unordered(download_agent, failures)):\n",
        "            if result is not None:\n",
        "                status = result.get('status')\n",
        "                task = result.get('task')\n",
        "                downloaded = int(task['size'])\n",
        "                if status == 0:\n",
        "                    downloaded_size += downloaded\n",
        "                    pbar.set_description('[%.3f/%.3f GB] Downloading %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                    downloads.append(task)\n",
        "                    del failures[failures.index(task)]\n",
        "                elif status == 1:\n",
        "                    pbar.set_description('[%.3f/%.3f GB] Fail downloading %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                elif status == -1:\n",
        "                    downloaded_size += downloaded\n",
        "                    pbar.set_description('[%.3f/%.3f GB] Skipping %s' % (get_filesize(downloaded_size), total_size, task.get('filename')))\n",
        "                    skips.append(task)\n",
        "                else:\n",
        "                    pass\n",
        "            pbar.update()\n",
        "\n",
        "pool.close()\n",
        "pool.terminate()\n",
        "\n",
        "print('\\n\\n##################################')\n",
        "print('# Summary                        #')\n",
        "print('##################################')\n",
        "print('Tasks     :', len(tasks))\n",
        "print('-----------------')\n",
        "print('Downloads :', len(downloads))\n",
        "print('Skip      :', len(skips))\n",
        "print('Failures  :', len(failures))\n",
        "print('Errors    :', len(errors))\n",
        "\n",
        "if len(failures) > 0:\n",
        "    print('\\nWait for 1 minute and Re-run this section again to download all fail tasks')\n",
        "    for f in failures:\n",
        "        print(f['url'])\n",
        "\n",
        "if len(errors) > 0:\n",
        "    print('\\nError tasks')\n",
        "    for f in errors:\n",
        "        print(f['url'])\n",
        "\n",
        "print('\\nAll done, Voila!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}